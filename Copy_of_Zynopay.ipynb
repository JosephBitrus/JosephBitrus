{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JosephBitrus/JosephBitrus/blob/main/Copy_of_Zynopay.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "import openai\n",
        "import os\n",
        "from gpt_index import SimpleDirectoryReader, GPTSimpleVectorIndex, LLMPredictor, PromptHelper\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "import json\n",
        "\n",
        "app = Flask(__name__)\n",
        "\n",
        "# Set your OpenAI API key\n",
        "os.environ[\"OPENAI_API_KEY\"] = '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'\n",
        "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "# Context information for the chatbot\n",
        "context = 'you are talking with a customer from Zynopay who wants to find one product or business.give them one product, product description and the link. Do not end with puncuation after the link'\n",
        "\n",
        "# Function to construct the index for the AI model\n",
        "def construct_index(directory_path):\n",
        "    # Parameters for index construction\n",
        "    max_input_size = 8192\n",
        "    num_outputs = 1024\n",
        "    max_chunk_overlap = 20\n",
        "    chunk_size_limit = 600\n",
        "\n",
        "    # Initialize the PromptHelper to handle long texts\n",
        "    prompt_helper = PromptHelper(max_input_size, num_outputs, max_chunk_overlap, chunk_size_limit=chunk_size_limit)\n",
        "\n",
        "    # Initialize the language model predictor\n",
        "    llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\", max_tokens=num_outputs))\n",
        "\n",
        "    # Load documents from the provided directory path\n",
        "    documents = SimpleDirectoryReader(directory_path).load_data()\n",
        "\n",
        "    # Create the GPT index using the documents and predictors\n",
        "    index = GPTSimpleVectorIndex(documents, llm_predictor=llm_predictor, prompt_helper=prompt_helper)\n",
        "\n",
        "    # Save the index to disk for later use\n",
        "    index.save_to_disk('index.json')\n",
        "\n",
        "    return index\n",
        "\n",
        "# Load the pre-constructed index from disk\n",
        "index = construct_index(\"C:/工作/Company_project/Zynopay/\")\n",
        "\n",
        "# Function to handle chatbot API requests\n",
        "@app.route('/chatbot', methods=['POST'])\n",
        "def chatbot_api(user_id = \"default_user\"):\n",
        "    data = request.get_json()\n",
        "\n",
        "    if not data or 'message' not in data:\n",
        "        return jsonify({'error': 'Invalid request format'}), 400\n",
        "\n",
        "    user_input = data['message']\n",
        "\n",
        "    # Check if the user wants to end the conversation\n",
        "    if user_input.lower() in [\"exit\", \"quit\", \"bye\", \"end\"]:\n",
        "        return jsonify({'response': 'Goodbye! Have a great day!'})\n",
        "\n",
        "    # Context information for the chatbot\n",
        "    try:\n",
        "        with open('conversation_history.json', 'r') as file:\n",
        "            user_conversations = json.load(file)\n",
        "    except FileNotFoundError:\n",
        "        user_conversations = {}  # Dictionary to store user conversations\n",
        "\n",
        "    # Retrieve existing conversation context for the user\n",
        "    user_context = user_conversations.get(user_id, context)\n",
        "\n",
        "    # Add context to the user's input text\n",
        "    input_text_with_context = f\"{user_context}\\n\\nUser: {user_input}\"\n",
        "\n",
        "    # Query the index to generate a single response from the training data only\n",
        "    response = index.query(input_text_with_context, response_mode=\"compact\")\n",
        "    generated_response = response.response.strip()\n",
        "\n",
        "    # Update user conversation context\n",
        "    user_conversations[user_id] = f\"{user_context}\\n\\nUser: {user_input}\\nChatbot: {generated_response}\"\n",
        "\n",
        "    # Save the updated conversation history to file\n",
        "    with open('conversation_history.json', 'w') as file:\n",
        "        json.dump(user_conversations, file)\n",
        "\n",
        "    return jsonify({'response': generated_response})\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.run(host='0.0.0.0', port=5000)"
      ],
      "metadata": {
        "id": "YGudIw4nqt-W"
      },
      "id": "YGudIw4nqt-W",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}